{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score,precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "# sklearn.grid_search import GridSearchCV #esiste nella versione vecchia, cambiato nome prolly\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_base = pd.read_csv(\"Our_dataset/StemmedWord2vecTop3_parsed_weather_labeled.csv\", index_col=0) \n",
    "wine_base = wine_base.reset_index()\n",
    "wine_base= wine_base[pd.notnull(wine_base['description'])]\n",
    "wine_base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 5))\n",
    "sns.distplot(wine_base[\"points\"],hist=True,bins = 20,hist_kws={'edgecolor':'black'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE NUMBER OF BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right number of bins\n",
    "\n",
    "c= 1 + (10/3)*math.log10(20)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = wine_base['points'].copy()\n",
    "#DECIDE NUMBER OF BINS \n",
    "\n",
    "nbins  = 3\n",
    "labels=[\"low\",\"medium\",\"high\"]\n",
    "#nbins  = 4\n",
    "#labels=[\"low\",\"medium\",\"high\",\"very_high\"]\n",
    "#nbins = 5\n",
    "#labels=[\"very_low\", \"low\", \"medium\",\"high\",\"very_high\"]\n",
    "#bin identici \n",
    "#Y,bins = pd.cut(Y,nbins,labels=labels,retbins=True,include_lowest=True,right=True)\n",
    "#quartile\n",
    "Y,bins = pd.qcut(Y,nbins,labels=labels,retbins=True)  \n",
    "values = Y.tolist()  \n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 5))\n",
    "for i in range(1,len(bins)-1):\n",
    "\n",
    "    if i == 1:\n",
    "        a = wine_base[wine_base[\"points\"] <= bins[i]]\n",
    "        n =  bins[i]-80\n",
    "        sns.distplot(a[\"points\"],hist_kws={\"width\": 0.5,'edgecolor':'black'},kde=False)\n",
    "        g =+n\n",
    "    if i == len(bins)-1:\n",
    "        a = wine_base[wine_base[\"points\"] > bins[i]]\n",
    "        n = 100 - bins[i]\n",
    "        sns.distplot(a[\"points\"],hist_kws={\"width\": 0.5,'edgecolor':'black'},kde=False)\n",
    "        g =+ n\n",
    "    else:\n",
    "        n = bins[i+1] - bins[i]\n",
    "        g =+n\n",
    "        a = wine_base[(wine_base[\"points\"] > bins[i]) & (wine_base[\"points\"] <= bins[i+1])]\n",
    "        sns.distplot(a[\"points\"],hist_kws={\"width\": 0.5,'edgecolor':'black'},kde=False)\n",
    "ax.set(xticks=wine_base[\"points\"].unique())\n",
    "print(g)\n",
    "sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic = [\"price\",'vintage', 'country', 'province', 'region_1', 'taster_name', 'variety','winery']\n",
    "word = [\"word_count\"]\n",
    "tfGroup = ['tf_grouped_1','tf_grouped_2', 'tf_grouped_3']\n",
    "tfIdfGroup = ['tfIdf_grouped_1', 'tfIdf_grouped_2', 'tfIdf_grouped_3']\n",
    "tfFull = ['tf_fullData_1', 'tf_fullData_2', 'tf_fullData_3',]\n",
    "tfIdfFull = ['tfIdf_fullData_1', 'tfIdf_fullData_2', 'tfIdf_fullData_3']\n",
    "#weather = ['pr_5', 'pr_6', 'pr_7', 'pr_8', 'pr_9', 'tas_5', 'tas_6', 'tas_7', 'tas_8', 'tas_9']# don't really care bcs 0 improvements\n",
    "word2vec = ['similarityTop3WinesByVariety']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = basic + word + tfGroup + word2vec\n",
    "X = wine_base.loc[:,features]\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMMY CLASSIFIER = BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = wine_base.loc[:,features]\n",
    "test_size = 0.30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "#classifier = DecisionTreeClassifier()\n",
    "classifier = DummyClassifier(\"stratified\")\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)  \n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "weightedPrec = precision_score(y_test,y_pred,average=\"weighted\")\n",
    "#get feature importances\n",
    "#lista\n",
    "#for name, importance in zip(features, classifier.feature_importances_):\n",
    " #   lista.append([name, importance])\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"dummyClassifier.txt\",\"a\")\n",
    "file.write(\"Decision Tree Classifier with points divided into \" + str(bins) + \" bins \\n\")\n",
    "file.write(\"Built on: \" + str(features) + \"\\n\")\n",
    "file.write(\"Test size: \" + str(test_size) + \"\\n\")\n",
    "file.write(\"   \" +\"accuracy \" + str(acc) + \"\\n\")\n",
    "file.write(\"   \" +\"weightedPrec \" + str(weightedPrec) + \"\\n\")\n",
    "#file.write(\"Feature importances: \\n\")\n",
    "#for el in lista: \n",
    "#file.write(\"   \" + el[0] + \":   \" + str(el[1]) + \"\\n\")\n",
    "file.write(\"Report By predicted class: \\n\")\n",
    "file.write(classification_report(y_test, y_pred, target_names=labels))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"decTree\" #\"randomForest\" o \"decTree\"\n",
    "features = word\n",
    "X = wine_base.loc[:,features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm == \"decTree\":\n",
    "    parameters={'max_depth': np.arange(2,10)}#'min_samples_split' : np.arange(10,500,20)}\n",
    "    clf_tree=DecisionTreeClassifier()\n",
    "else: \n",
    "    parameters={'max_depth': np.arange(2,7),\"n_estimators\":[2,3,4,5,6,7,8,9,10]}#'min_samples_split' : np.arange(10,500,20)}\n",
    "    clf_tree = RandomForestClassifier()\n",
    "\n",
    "clf= GridSearchCV(clf_tree,parameters)\n",
    "clf.fit(X,Y)\n",
    "print(\"Best parameters for \" + algorithm + \"points variable binnned by: \" + str(bins))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 4\n",
    "estimators = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"randomForest\"\n",
    "test_size = 0.30 #non va in overfitting, si vede dal grafico sotto\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "if algorithm == \"decTree\":\n",
    "    classifier = DecisionTreeClassifier(max_depth=depth)\n",
    "    print(\"Results obtained by \" + algorithm + \" with depth \" + str(depth))\n",
    "else:\n",
    "    classifier = RandomForestClassifier(max_depth=depth,n_estimators = estimators)\n",
    "    print(\"Results obtained by \" + algorithm + \" with depth \" + str(depth) + \"n_estimators = \" +str(estimators))\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)  \n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "weightedPrec = precision_score(y_test,y_pred,average=\"weighted\")\n",
    "\n",
    "#get feature importances\n",
    "lista = []\n",
    "for name, importance in zip(features, classifier.feature_importances_):\n",
    "    lista.append([name, importance])\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(y_test, y_pred,labels=labels)\n",
    "r = classification_report(y_test, y_pred, target_names=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "model = SelectFromModel(classifier, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "feature_idx = model.get_support()\n",
    "temp = []\n",
    "for i in range(0,len(feature_idx)): \n",
    "    if feature_idx[i]:\n",
    "        temp.append(i)\n",
    "print(temp)\n",
    "selFeatures = []\n",
    "for i in range(0,len(temp)):\n",
    "    selFeatures.append(features[temp[i]])\n",
    "selFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=test_size, random_state=42)\n",
    "if algorithm == \"decTree\":\n",
    "    classifier=DecisionTreeClassifier(max_depth=depth)\n",
    "    print(\"Results obtained by \" + algorithm + \" with depth \" + str(depth))\n",
    "else: \n",
    "    classifier = RandomForestClassifier(max_depth=depth,n_estimators = estimators)\n",
    "    print(\"Results obtained by \" + algorithm + \" with depth \" + str(depth) + \"and N_estimators = \" + str(estimators))\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred =classifier.predict(X_test)  \n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "weightedPrec = precision_score(y_test,y_pred,average=\"weighted\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "def conda_fix(graph):\n",
    "        path = os.path.join(sys.base_exec_prefix, \"Library\", \"bin\", \"graphviz\")\n",
    "        paths = (\"dot\", \"twopi\", \"neato\", \"circo\", \"fdp\")\n",
    "        paths = {p: os.path.join(path, \"{}.exe\".format(p)) for p in paths}\n",
    "        graph.set_graphviz_executables(paths)\n",
    "import pydotplus \n",
    "from sklearn import tree\n",
    "from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(classifier, out_file=None, \n",
    "                         feature_names=selFeatures,  \n",
    "                         class_names=labels,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "conda_fix(graph)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check AUC, feature selection, and data with sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same only add everything into a data frame so that you already have there everything you can plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"classification\" + str(bins) + \"bins.txt\",\"a\")\n",
    "file.write(\"Decision Tree Classifier with points divided into \" + str(bins) + \" bins \\n\")\n",
    "file.write(\"Built on: \" + str(features) + \"\\n\")\n",
    "file.write(\"Test size: \" + str(test_size) + \"\\n\")\n",
    "file.write(\"   \" +\"accuracy \" + str(acc) + \"\\n\")\n",
    "file.write(\"   \" +\"weightedPrec \" + str(weightedPrec) + \"\\n\")\n",
    "file.write(\"Feature importances: \\n\")\n",
    "for el in lista: \n",
    "    file.write(\"   \" + el[0] + \":   \" + str(el[1]) + \"\\n\")\n",
    "file.write(\"Report By predicted class: \\n\")\n",
    "file.write(classification_report(y_test, y_pred, target_names=labels))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(DecisionTreeClassifier(max_depth=3), \n",
    "                                                        X, \n",
    "                                                        Y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=10,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "plt.subplots(figsize = (12, 5))\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"red\",alpha = 0.5)\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"blue\",alpha = 0.5)\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"price\"],#[\"price\",\"word_count\"],basic,basic+tfGroup, basic+word2vec,basic+word2vec+tfGroup,word2vec,tfGroup,word2vec+tfGroup]\n",
    "allfeatures = basic + word + word2vec + tfGroup \n",
    "decTreeCombinations = {\"depth\":[2,3,4,5],\"args\":args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modificala per usare anche il random forest a modo\n",
    "def testToDataFrame(algorithm,combination,Y,allfeats,dataset):\n",
    "    test_size = 0.30 #non va in overfitting, si vede dal grafico sotto\n",
    "    cols = [\"algorithm\",\"input\",\"precision\",\"accuracy\",\"depth\"]\n",
    "    for lab in labels:\n",
    "        cols.append(lab +\"_prec\")\n",
    "        cols.append(lab +\"_recall\")\n",
    "        cols.append(lab +\"_f1\")\n",
    "        cols.append(lab +\"_support\")\n",
    "    for el in features:\n",
    "        cols.append(\"feat_\"+el)\n",
    "    results = pd.DataFrame()\n",
    "    row = 0\n",
    "    comb = 0\n",
    "    for el in combination[\"args\"]: \n",
    "        for depth in combination[\"depth\"]:\n",
    "            row = row + 1\n",
    "            X = dataset.loc[:,el]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "            if algorithm == \"decTree\":\n",
    "                classifier = DecisionTreeClassifier(max_depth=depth)\n",
    "            else:\n",
    "                classifier = RandomForestClassifier(max_depth=depth,n_estimators = estimators)\n",
    "            classifier.fit(X_train,y_train)\n",
    "            y_pred = classifier.predict(X_test)  \n",
    "            acc = accuracy_score(y_test,y_pred)\n",
    "            weightedPrec = precision_score(y_test,y_pred,average=\"weighted\")        \n",
    "            data = [algorithm,el,weightedPrec,acc,depth]\n",
    "            precision,recall,fscore,support = precision_recall_fscore_support(y_test, y_pred)\n",
    "            for i in range(0,len(labels)):\n",
    "                data.append(precision[i])\n",
    "                data.append(recall[i])\n",
    "                data.append(fscore[i])\n",
    "                data.append(support[i])\n",
    "            #for lab in labels:\n",
    "            \n",
    "             #   data.append(rep[lab].precision)     #ORDER IS VERY IMPORTANT\n",
    "              #  data.append(rep[lab].recall)\n",
    "               # data.append(rep[lab].f1-score)\n",
    "                #data.append(rep[lab].support)\n",
    "            temp = {}\n",
    "            c = zip(el,classifier.feature_importances_)\n",
    "            for name,importance in c:\n",
    "                temp[name] = importance\n",
    "            for feat in allfeats:\n",
    "                if feat not in el:\n",
    "                    data.append(100) #100 is an impossible value not to be taken into account\n",
    "                else:\n",
    "                    data.append(temp[feat])\n",
    "            df2 = pd.DataFrame([data],columns=cols)\n",
    "            results = results.append(df2,ignore_index=True)\n",
    "        comb = comb + 1\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = testToDataFrame(\"decTree\",decTreeCombinations,Y,allfeatures,wine_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values([\"precision\",'depth'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
